# 幾何学変換



## アフィン変換

```C++
#include <opencv2/opencv.hpp>
 
int main(void)
{
	cv::Mat	src, dst;
 
	src = cv::imread("C:/opencv/sources/samples/data/lena.jpg");
 
	// アフィン変換行列(2行3列）
	cv::Mat	affine = (cv::Mat_<double>(2, 3) << 0.8, 0, 0, 0, 1.5, 0);
 
	// アフィン変換
	// # warpAffine(原画像, 変換後画像, 変換後画像サイズ, 補間方法)
	// # INTER_LINEAR  線形補間（バイリニア）
	//   INTER_NEAREST 再近傍補間
	//   INTER_CUBIC   バイキュービック補間
	//   INTER_LANCZOS4 Lanczos（ランチョス）補間
	cv::warpAffine(src, dst, affine, src.size(), cv::INTER_LINEAR);
 
	cv::imshow("原画像", src);
	cv::imshow("変換後画像", dst);
 
	cv::waitKey();
 
	return 0;
}
```



### 課題

画像を斜めに平行移動させよ。
画像を中央を中心に拡大させよ。
画像の中央を中心に回転させよ。



答:

```C++
#include<opencv2/core/core.hpp>
#include<opencv2/highgui/highgui.hpp>
#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/opencv.hpp"
#include<iostream>
#include<math.h>
using namespace std;
using namespace cv;
string path = "C:\\Users\\admin\\Desktop\\openCVTestFiles\\";

int main(void)
{
	Mat puni = imread(path + "A17_puni.png");
	Mat Ans1, Ans2, Ans3;
	Mat affine1 = (Mat_<double>(2, 3) << 1, 0, 50, 0, 1, 50);
	cout << affine1 << endl;
	warpAffine(puni, Ans1, affine1, puni.size(), INTER_LINEAR);
	imshow("origin", puni);
	imshow("affine1", Ans1);

	Point center = Point(puni.size().width / 2, puni.size().height / 2);
	Mat affine2 = getRotationMatrix2D(center, 0, 2);
	cout << affine2 << endl;
	warpAffine(puni, Ans2, affine2, puni.size(), INTER_LINEAR);
	imshow("affine2", Ans2);

	double angle = -90;
	Mat affine3 = getRotationMatrix2D(center, angle, 1);
	cout << affine3 << endl;
	warpAffine(puni, Ans3, affine3, puni.size(), INTER_LINEAR);
	imshow("affine3", Ans3);

	waitKey(0);
	return 0;
}
```



以下為解題思路

東西都忘光了對不起我的線代教授

參考[官方文件](https://docs.opencv.org/3.4/d4/d61/tutorial_warp_affine.html)進行複習

這次的線性映射使用的是一個2X3的矩陣，實際上是兩個矩陣分別為2X2和2X1

比如範例中的`(cv::Mat_<double>(2, 3) << 0.8, 0, 0, 0, 1.5, 0)`

```
[0.8, 0,  0;
 0,  1.5, 0]
```

實際上應該看成

```
[0.8, 0;
 0,   1.5]
```

和

```
[0;
 0]
```

暫且把第一個2X2矩陣作A 第二個2X1矩陣作B

假設有一個座標點X=[x, y]T (2X1矩陣)

映射結果應為 AX+B

以此範例為例的話就是[0.8x,1.5y]T



OK，搞清楚映射方式後就可以來解答了



第一題基本上沒甚麼問題，就是把數值填入矩陣B就能直接作位移了

第三題一開始想偏了以原點為中心作旋轉後再把圖片位移到看的見的地方，如下

```C++
const double pi = 3.1415926;
double rotationDegrees = 90;
double para = rotationDegrees * pi / 180;
Mat affine3 = (Mat_<double>(2, 3) << cos(para), -sin(para), puni.size().width, sin(para), cos(para), 0);
```

這算是偷吃步了，而且只要角度一改動位移量也要跟著改，顯然不是一個好做法

後來仔細想想，我不如先把原矩陣的中心移動到原點的位置，旋轉完成後再次位移回到原來的位置

想法如下

```
[a11,a12; X [x-(1/2)width;  + [(1/2)width;
 a21,a22]    y-(1/2)height]    (1/2)height]
```

實現如下

```C++
	const double pi = 3.1415926;
	double rotationDegrees = 90;
	double para = rotationDegrees * pi / 180;
	Mat affine3 = (Mat_<double>(2, 3) << cos(para), -sin(para), puni.size().width / 2, sin(para), cos(para), puni.size().height / 2);
	cout << affine3 << endl;
	Mat movedPuni;
	Mat affineMovedPuni = (Mat_<double>(2, 3) << 1, 0, -puni.size().width / 2, 0, 1, -puni.size().height / 2);
	warpAffine(puni, movedPuni, affineMovedPuni, puni.size(), INTER_LINEAR);
	warpAffine(movedPuni, Ans3, affine3, puni.size(), INTER_LINEAR);
```

結果實際執行的時候由於第一次位移之後圖片中座標為負的部分不會被`CV::Mat`存到，所以只會保留到右下部分(座標正值)，導致最終結果也只剩下一個角落。

後來看文件發現openCV可以直接生成旋轉矩陣，那就用現成的吧，順便一提順時針90度的矩陣生成如下

```
[6.123233995736766e-17, -1, 300;
 1, 6.123233995736766e-17, -2.842170943040401e-14]
```

跟我一開始寫得差不多= =，不過當然好處在於它可以任意選擇角度，如果以我最初的做法的話以原點為中心旋轉完後還得另外計算位移量。

最後第二題就直接用第三題的旋轉方法改scale完成。



## 対応点からアフィン変換行列を算出

```C++
#include <opencv2/opencv.hpp>
 
int main(void)
{
	cv::Mat	src, dst;
 
	src = cv::imread("C:/opencv/sources/samples/data/lena.jpg");
 
	// アフィン変換行列
	cv::Mat	affine;
 
	// 対応点（3点）の設定
	// # 0.8f の f は直前の数値がfloat型であることを示す。
	//   fをつけない実数定数はdouble型になる。
	cv::Point2f srcPoint[3] = { { 0, 0 },{ 1, 0 },{ 0, 1 } };
	cv::Point2f dstPoint[3] = { { 0, 0 },{ 0.8f, 0.2f },{ 0.2f, 0.8f } };
 
	// 対応点からアフィン変換行列を求める
	// # srcPoint から dstPoint への変換を行うアフィン変換行列が得られる。
	affine = cv::getAffineTransform(srcPoint, dstPoint);
 
	// アフィン変換
	cv::warpAffine(src, dst, affine, src.size(), cv::INTER_LINEAR);
 
	cv::imshow("原画像", src);
	cv::imshow("変換後画像", dst);
 
	cv::waitKey();
 
	return 0;
}
```



### 心得

很方便的功能，拿這個來試試上面的[第二題](#課題)

```C++
#include<opencv2/core/core.hpp>
#include<opencv2/highgui/highgui.hpp>
#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/opencv.hpp"
#include<iostream>
#include<math.h>
using namespace std;
using namespace cv;
string path = "C:\\Users\\admin\\Desktop\\openCVTestFiles\\";

int main(void)
{
	Mat puni = imread(path + "A17_puni.png");
	
	Point2f ori[3] = { {(float)puni.size().width / 2,(float)puni.size().height / 2},
		{(float)puni.size().width / 2 + 1,(float)puni.size().height / 2} ,
		{(float)puni.size().width / 2,(float)puni.size().height / 2 + 1} };

	Point2f map[3] = { {(float)puni.size().width / 2,(float)puni.size().height / 2},
		{(float)puni.size().width / 2 + 1.2f,(float)puni.size().height / 2} ,
		{(float)puni.size().width / 2,(float)puni.size().height / 2 + 1.2f} };

	Mat affine = getAffineTransform(ori, map);
	cout << affine << endl;

	Mat res;
	warpAffine(puni, res, affine, puni.size(), INTER_LINEAR);

	imshow("ori", puni);
	imshow("res", res);
	waitKey(0);
	return 0;
}
```

比較特別要注意的是型別問題

`getAffineTransform`的參數只接受`Point2f`不接受`Point2d`

`Point2f`的資料必須為`float`，上面程式碼為例`puni.size().width / 2`為`int`(因為`puni.size().width`是`int`)所以我在前方加入`(float)`強制轉型，`1.2`會被認為`double`所以改寫為`1.2f`



當然第三題也可以

```C++
#include<opencv2/core/core.hpp>
#include<opencv2/highgui/highgui.hpp>
#include "opencv2/imgproc/imgproc.hpp"
#include "opencv2/opencv.hpp"
#include<iostream>
#include<math.h>
using namespace std;
using namespace cv;
string path = "C:\\Users\\admin\\Desktop\\openCVTestFiles\\";

int main(void)
{
	Mat puni = imread(path + "A17_puni.png");

	Point2f ori[3] = { {(float)puni.size().width / 2,(float)puni.size().height / 2},
		{(float)puni.size().width / 2 + 1,(float)puni.size().height / 2} ,
		{(float)puni.size().width / 2,(float)puni.size().height / 2 + 1} };

	Point2f map[3] = { {(float)puni.size().width / 2,(float)puni.size().height / 2},
		{(float)puni.size().width / 2,(float)puni.size().height / 2 + 1} ,
		{(float)puni.size().width / 2 - 1,(float)puni.size().height / 2 } };

	Mat affine = getAffineTransform(ori, map);
	cout << affine << endl;

	Mat res;
	warpAffine(puni, res, affine, puni.size(), INTER_LINEAR);

	imshow("ori", puni);
	imshow("res", res);
	waitKey(0);
	return 0;
}
```





## 射影変換

```C++
#include <opencv2/opencv.hpp>
 
int main(void)
{
	cv::Mat	src, dst;
 
	src = cv::imread("C:/opencv/sources/samples/data/lena.jpg");
 
	// ホモグラフィ行列(3行3列）
	cv::Mat	homography = (cv::Mat_<double>(3, 3) << 0.8, -0.2, 180, 0.4, 0.9, 40, 0.001, 0.0001, 1);
 
	// 射影変換
	cv::warpPerspective(src, dst, homography, src.size(), cv::INTER_LINEAR);
 
	cv::imshow("原画像", src);
	cv::imshow("変換後画像", dst);
 
	cv::waitKey();
 
	return 0;
}
```



[Wiki說明](https://zh.wikipedia.org/wiki/%E5%8D%95%E5%BA%94%E6%80%A7)

## 立方体の面への画像の貼り付け

```C++
#include <opencv2/opencv.hpp>
 
int main(void)
{
	cv::Mat	src, dst;
 
	src = cv::imread("C:/opencv/sources/samples/data/lena.jpg");
 
	// 射影変換行列
	cv::Mat	homography1, homography2, homography3;
 
	// 対応点の設定
	// # destPointは、立方体の三つの面の各頂点を2次元平面に投影した座標
	float w = src.cols - 1, h = src.rows - 1;
	cv::Point2f srcPoint[4] = { { 0, 0 },{ w, 0 },{ w, h },{ 0, h } };
	cv::Point2f dstPoint1[4] = { { 25, 100 },{ 230, 45 },{ 362, 60 },{ 195, 150 } };
	cv::Point2f dstPoint2[4] = { { 25, 100 },{ 195, 150 },{ 225, 370 },{ 85, 285 } };
	cv::Point2f dstPoint3[4] = { { 195, 150 },{ 362, 60 },{ 350, 215 },{ 225, 370 } };
 
 
	// 対応点から射影変換行列を求める（srcPoint → dstPoint1）
	homography1 = cv::getPerspectiveTransform(srcPoint, dstPoint1);
	homography2 = cv::getPerspectiveTransform(srcPoint, dstPoint2);
	homography3 = cv::getPerspectiveTransform(srcPoint, dstPoint3);
 
	// 射影変換
	// # BORDER_TRANSPARENT  変換後画像を初期化しないで上書きする
	cv::warpPerspective(src, dst, homography1, cv::Size(400, 400), cv::INTER_LINEAR);
	cv::warpPerspective(src, dst, homography2, cv::Size(400, 400), cv::INTER_LINEAR, cv::BORDER_TRANSPARENT);
	cv::warpPerspective(src, dst, homography3, cv::Size(400, 400), cv::INTER_LINEAR, cv::BORDER_TRANSPARENT);
 
	// 立方体の辺を描く
	for (int i = 0; i < 4; i++) {
		cv::line(dst, dstPoint1[i], dstPoint1[(i + 1) & 3], cv::Scalar::all(255), 1, cv::LINE_AA);
		cv::line(dst, dstPoint2[i], dstPoint2[(i + 1) & 3], cv::Scalar::all(255), 1, cv::LINE_AA);
		cv::line(dst, dstPoint3[i], dstPoint3[(i + 1) & 3], cv::Scalar::all(255), 1, cv::LINE_AA);
	}
 
	cv::imshow("原画像", src);
	cv::imshow("立方体へのマッピング", dst);
 
	cv::waitKey();
 
	return 0;
}
```

## マウスを使って画像を自由に変形する

```C++
#include <opencv2/opencv.hpp>

// 射影変換のための各情報を保存する構造体
struct ImageInfo {
	cv::Mat src, dst;		// 入力画像と出力画像
	cv::Mat matrix;			// 射影変換行列
	cv::Point2f srcPt[4];	// 変換前の4頂点座標（左上, 右上, 右下, 左下）
	cv::Point2f dstPt[4];	// 変換後の4頂点座標（左上, 右上, 右下, 左下）
	std::string winName;	// 出力ウインドウの名前
};

// コールバック関数
void mouseCallback(int event, int x, int y, int flags, void *data)
{
	static int select = -1;		// マウスで選択された頂点番号（-1:選択無し）
	cv::Point2f p(x, y);		// マウスの座標
	double dis = 1e10;

	ImageInfo &info = *(ImageInfo *)data;

	switch (event) {
	case cv::EVENT_LBUTTONDOWN:
		// 左ボタンを押したとき、4頂点のうち一番近い点を探す
		for (int i = 0; i < 4; i++) {
			double d = cv::norm(p - info.dstPt[i]);	// 頂点iとマウス座標との距離を計算
			if (d < 20 && d < dis) {
				select = i;
				dis = d;
			}
		}
		break;

	case cv::EVENT_RBUTTONDOWN:
		// 右ボタンを押したとき、ホモグラフィ行列を出力する
		std::cout << info.matrix << std::endl;
		break;

	case cv::EVENT_LBUTTONUP:
		// 左ボタンを離したとき、画像を射影変換してウインドウに表示する

		// 変換前後の座標からホモグラフィ行列を求める
		info.matrix = cv::getPerspectiveTransform(info.srcPt, info.dstPt);
		// 射影変換をする
		cv::warpPerspective(info.src, info.dst, info.matrix, info.dst.size(), cv::INTER_LINEAR);
		// ウインドウに表示する
		cv::imshow(info.winName, info.dst);

		select = -1;	
		break;
	}

	if (flags & cv::EVENT_FLAG_LBUTTON && select > -1) {
		// マウスの左ボタンが押されている、かつ、頂点が選択されているとき、
		// 選択されている頂点の座標を現在のマウスの位置にする
		info.dstPt[select] = p;

		// 画像範囲の枠を描く

		// 4頂点座標を cv::Point（整数座標）の vector に変換する
		// # polylines が cv::Point2f に対応していないため。
		// # getPerspectiveTransform は cv::Point2f にしか対応していない。
		std::vector<cv::Point> poly;
		for (int i = 0; i < 4; i++) {
			poly.push_back(cv::Point(info.dstPt[i]));
		}
		cv::Mat overlay(info.dst.size(), CV_8UC3);
		cv::polylines(overlay, poly, true, cv::Scalar::all(255), 3, cv::LINE_AA);

		// 変換後画像に枠線を重ねて表示する
		cv::imshow(info.winName, info.dst + overlay);
	}
}

int main(void)
{
	// ウインドウのサイズ（変更可）
	cv::Size window(800, 800);

	// 射影変換のための画像情報構造体
	ImageInfo info;

	// 画像を読み込む
	info.src = cv::imread("C:/opencv/sources/samples/data/butterfly.jpg", cv::IMREAD_COLOR);
	info.dst = cv::Mat(info.src.size(), CV_8UC3);

	// 原画像の4頂点座標
	info.srcPt[0] = cv::Point2f(0, 0);
	info.srcPt[1] = cv::Point2f(info.src.cols - 1, 0);
	info.srcPt[3] = cv::Point2f(0, info.src.rows - 1);
	info.srcPt[2] = cv::Point2f(info.src.cols - 1, info.src.rows - 1);

	// 画像の縮小率を計算する（ウインドウ幅の1/2のサイズにする）
	double scale;
	if (info.src.cols > info.src.rows) {
		scale = double(window.width) / info.src.cols / 2;
	} else {
		scale = double(window.height) / info.src.rows / 2;
	}

	// コールバック関数を登録する
	info.winName = "射影変換お試し";
	cv::namedWindow(info.winName);
	cv::setMouseCallback(info.winName, mouseCallback, (void *)&info);

	int key = 'r';
	do {
		if (key == 'r') {
			// 変換後の4頂点座標（初期位置。画像サイズをウインドウの1/2にしてウインドウの中央に表示する）
			info.dstPt[0] = cv::Point2f((window.width - info.src.cols * scale) / 2, (window.height - info.src.rows * scale) / 2);
			info.dstPt[1] = cv::Point2f(info.src.cols - 1, 0) * scale + info.dstPt[0];
			info.dstPt[3] = cv::Point2f(0, info.src.rows - 1) * scale + info.dstPt[0];
			info.dstPt[2] = cv::Point2f(info.src.cols - 1, info.src.rows - 1) * scale + info.dstPt[0];

			// 対応点から射影変換行列を求める（srcPoint → dstPoint1）
			info.matrix = cv::getPerspectiveTransform(info.srcPt, info.dstPt);

			// 射影変換をして画像を表示する
			cv::warpPerspective(info.src, info.dst, info.matrix, window, cv::INTER_LINEAR);
			cv::imshow(info.winName, info.dst);
		}

		key = cv::waitKey();
	} while (key != 0x1b);

	return 0;
}
```

